# ultra_sae_bayes.yaml  – launch with:  wandb sweep ultra_sae_bayes.yaml
program: splatnlp/monosemantic_sae/cli.py

method: bayes
metric:
  name: val_miracle_distance
  goal: minimize

early_terminate:
  type: hyperband
  min_iter: 3
  max_iter: 10
  s: 2

parameters:
  # -----------------------------------------------------------------------
  # 0.  FIXED START VALUE  (no sweep)
  # -----------------------------------------------------------------------
  l1-start:
    value: 1e-6                 # always begin almost pressure‑free

  # -----------------------------------------------------------------------
  # 1.  MODEL CAPACITY
  # -----------------------------------------------------------------------
  expansion-factor:
    values: [32, 40, 48]

  # -----------------------------------------------------------------------
  # 2.  SPARSITY CONTROL  (ceiling & warm‑up)
  # -----------------------------------------------------------------------
  l1-coeff:                     # ceiling after warm‑up
    distribution: uniform
    min: 0.00010
    max: 0.00030

  l1-warmup-steps:
    values: [0, 20000]

  # -----------------------------------------------------------------------
  # 3.  USAGE BALANCING
  # -----------------------------------------------------------------------
  usage-coeff:
    values: [0.0, 0.05]

  # -----------------------------------------------------------------------
  # 4.  OPTIMISER (fixed for this sweep)
  # -----------------------------------------------------------------------
  lr:
    value: 1e-4

  # -----------------------------------------------------------------------
  # 5.  REMAINING FIXED HYPERS
  # -----------------------------------------------------------------------
  target-usage:
    value: 0.007
  dead-neuron-threshold:
    value: 1e-6
  kl-warmup-steps:
    value: 0
  kl-floor:
    value: 0.0
  resample-weight:
    value: 0.01
  resample-bias:
    value: -1.0
  buffer-size:
    value: 100000
  steps-before-train:
    value: 5000

command:
  - ${env}
  - python
  - -m
  - splatnlp.monosemantic_sae.cli
  - --model-checkpoint=https://splat-nlp.nyc3.cdn.digitaloceanspaces.com/dataset_v2/model_ultra.pth
  - --data-csv=https://splat-nlp.nyc3.cdn.digitaloceanspaces.com/dataset_v2/tokenized_data.csv
  - --save-dir=sae_runs/${wandb.run.id}
  - --vocab-path=https://splat-nlp.nyc3.cdn.digitaloceanspaces.com/dataset_v2/vocab.json
  - --weapon-vocab-path=https://splat-nlp.nyc3.cdn.digitaloceanspaces.com/dataset_v2/weapon_vocab.json

  # fixed recipe flags
  - --epochs=10
  - --primary-data-fraction=0.005
  - --buffer-size=100000
  - --steps-before-train=5000

  # hyper‑parameters injected here
  - ${args}

  # misc
  - --device=cuda
  - --wandb-log

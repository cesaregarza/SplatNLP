{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SplatNLP Colab Demo\n",
        "\n",
        "Run-all notebook (~5\u20138 min) that demonstrates:\n",
        "\n",
        "- Multi-label set completion in token space\n",
        "- Constraint-aware decoding (beam search + exact allocator) \u2192 a legal gear build\n",
        "- Sparse Autoencoder (SAE) feature readouts (probe mode) + token influence\n",
        "\n",
        "Tip: switch to a GPU runtime if available; CPU works but is slower.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_URL = \"https://github.com/cesaregarza/SplatNLP.git\"\n",
        "REPO_DIR = Path(\"/content/SplatNLP\")\n",
        "\n",
        "if not REPO_DIR.exists():\n",
        "    subprocess.run(\n",
        "        [\"git\", \"clone\", \"--depth\", \"1\", REPO_URL, str(REPO_DIR)],\n",
        "        check=True,\n",
        "    )\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "print(\"Repo ready:\", REPO_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip -q install -e .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import splatnlp\n",
        "\n",
        "print(\"\u2705 Installed splatnlp (editable)\")\n",
        "print(\"splatnlp import:\", splatnlp.__file__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model at a glance\n",
        "\n",
        "SplatGPT is a SetTransformer-style model: embed ability tokens + weapon ID,\n",
        "process the (unordered) set with attention layers, pool to a 512-D vector,\n",
        "then predict every token independently (multi-label).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from IPython.display import Image, display\n",
        "\n",
        "img = Path(\"neural_network_themed.gv.png\")\n",
        "if img.exists():\n",
        "    display(Image(filename=str(img), width=460))\n",
        "else:\n",
        "    print(\"Diagram not found:\", img)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download artifacts (Ultra by default)\n",
        "\n",
        "This grabs `model_params.json`, `vocab.json`, `weapon_vocab.json`, and\n",
        "`model_info.json`, plus the model checkpoint.\n",
        "\n",
        "- `USE_ULTRA_MODEL=True` uses the Ultra checkpoint (`model_ultra.pth`).\n",
        "- `ENABLE_ULTRA_SAE=True` also downloads the Ultra SAE (and optional labels)\n",
        "  so the mech-interp section can run.\n",
        "\n",
        "Optional: if feature labels are available in the dataset directory\n",
        "(`feature_labels_ultra.json` or `consolidated_ultra.json`), the Ultra section\n",
        "will display human-readable names for top SAE features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from splatnlp.utils.download_artifacts import (\n",
        "    CORE_ARTIFACTS,\n",
        "    ULTRA_ARTIFACTS,\n",
        "    ULTRA_LABELS_ARTIFACTS,\n",
        "    ULTRA_SAE_ARTIFACTS,\n",
        "    download_artifacts,\n",
        ")\n",
        "\n",
        "BASE_URL = \"https://splat-nlp.nyc3.cdn.digitaloceanspaces.com\"\n",
        "DATASET_DIR = \"dataset_v2\"\n",
        "OUT_DIR = Path(\"saved_models\") / DATASET_DIR\n",
        "\n",
        "USE_ULTRA_MODEL = True\n",
        "ENABLE_ULTRA_SAE = True\n",
        "FORCE_DOWNLOAD = False\n",
        "VERBOSE_LIST = False\n",
        "\n",
        "# If we enable the Ultra SAE, we must use the Ultra backbone.\n",
        "if ENABLE_ULTRA_SAE:\n",
        "    USE_ULTRA_MODEL = True\n",
        "\n",
        "MODEL_FILENAME = \"model_ultra.pth\" if USE_ULTRA_MODEL else \"model.pth\"\n",
        "\n",
        "artifacts = list(CORE_ARTIFACTS)\n",
        "if USE_ULTRA_MODEL:\n",
        "    artifacts = [a for a in artifacts if a.filename != \"model.pth\"]\n",
        "    artifacts += list(ULTRA_ARTIFACTS)\n",
        "\n",
        "if ENABLE_ULTRA_SAE:\n",
        "    # SAE implies Ultra model.\n",
        "    if not USE_ULTRA_MODEL:\n",
        "        artifacts += list(ULTRA_ARTIFACTS)\n",
        "    artifacts += list(ULTRA_SAE_ARTIFACTS) + list(ULTRA_LABELS_ARTIFACTS)\n",
        "\n",
        "# De-dup by filename (preserve order).\n",
        "deduped = []\n",
        "seen = set()\n",
        "for spec in artifacts:\n",
        "    if spec.filename in seen:\n",
        "        continue\n",
        "    seen.add(spec.filename)\n",
        "    deduped.append(spec)\n",
        "artifacts = deduped\n",
        "\n",
        "download_artifacts(\n",
        "    base_url=BASE_URL,\n",
        "    dataset_dir=DATASET_DIR,\n",
        "    out_dir=OUT_DIR,\n",
        "    artifacts=artifacts,\n",
        "    force=FORCE_DOWNLOAD,\n",
        "    timeout_s=600,\n",
        "    quiet=False,\n",
        "    dry_run=False,\n",
        ")\n",
        "\n",
        "must_have = [\n",
        "    \"model_params.json\",\n",
        "    \"vocab.json\",\n",
        "    \"weapon_vocab.json\",\n",
        "    MODEL_FILENAME,\n",
        "]\n",
        "if ENABLE_ULTRA_SAE:\n",
        "    must_have += [\n",
        "        \"sae_config_ultra.json\",\n",
        "        \"sae_model_ultra.pth\",\n",
        "    ]\n",
        "\n",
        "missing = [name for name in must_have if not (OUT_DIR / name).exists()]\n",
        "if missing:\n",
        "    raise FileNotFoundError(f\"Missing artifacts after download: {missing}\")\n",
        "\n",
        "print(f\"\u2705 Artifacts OK ({len(must_have)} required) \u2192 {OUT_DIR}\")\n",
        "if VERBOSE_LIST:\n",
        "    for p in sorted(OUT_DIR.iterdir()):\n",
        "        print(\" -\", p.name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load model + run token-space inference\n",
        "\n",
        "This model is **multi-label set completion**: it outputs an independent probability for every token (not an autoregressive next-token LM).\n",
        "\n",
        "Tokenization note:\n",
        "- Standard abilities are represented as cumulative threshold tokens (e.g., 12 AP `run_speed_up` \u2192 `run_speed_up_3`, `_6`, `_12`).\n",
        "- Beam search below runs on capstones (highest tier per ability family), then expands them back to cumulative tokens when calling the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from splatnlp.model.models import SetCompletionModel\n",
        "from splatnlp.serve.tokenize import tokenize_build\n",
        "from splatnlp.utils.constants import (\n",
        "    BUCKET_THRESHOLDS,\n",
        "    MAIN_ONLY_ABILITIES,\n",
        "    NULL,\n",
        "    PAD,\n",
        "    STANDARD_ABILITIES,\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# Toggle: set True to print token-space tables (useful, but verbose).\n",
        "SHOW_TOKEN_TABLES = False\n",
        "\n",
        "params = json.loads((OUT_DIR / \"model_params.json\").read_text())\n",
        "vocab = json.loads((OUT_DIR / \"vocab.json\").read_text())\n",
        "weapon_vocab = json.loads((OUT_DIR / \"weapon_vocab.json\").read_text())\n",
        "\n",
        "pad_token_id = params.get(\"pad_token_id\", vocab[PAD])\n",
        "inv_vocab = {v: k for k, v in vocab.items()}\n",
        "\n",
        "model = SetCompletionModel(**params)\n",
        "model.load_state_dict(torch.load(OUT_DIR / MODEL_FILENAME, map_location=\"cpu\"))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(\n",
        "    f\"Model params: {n_params/1e6:.1f}M | \"\n",
        "    f\"vocab={len(vocab)} | weapons={len(weapon_vocab)}\"\n",
        ")\n",
        "\n",
        "weapon_id = (\n",
        "    \"weapon_id_8000\" if \"weapon_id_8000\" in weapon_vocab else next(iter(weapon_vocab))\n",
        ")\n",
        "\n",
        "partial_build = {\n",
        "    \"ink_saver_main\": 6,\n",
        "    \"run_speed_up\": 12,\n",
        "    \"intensify_action\": 12,\n",
        "}\n",
        "\n",
        "\n",
        "def build_to_capstones(build: dict[str, int]) -> list[str]:\n",
        "    capstones: list[str] = []\n",
        "    for ability in MAIN_ONLY_ABILITIES:\n",
        "        if build.get(ability):\n",
        "            capstones.append(ability)\n",
        "\n",
        "    for ability in STANDARD_ABILITIES:\n",
        "        ap = build.get(ability)\n",
        "        if ap is None:\n",
        "            continue\n",
        "        ap = int(ap)\n",
        "        thresh = max((t for t in BUCKET_THRESHOLDS if t <= ap), default=None)\n",
        "        if thresh is None:\n",
        "            continue\n",
        "        capstones.append(f\"{ability}_{thresh}\")\n",
        "\n",
        "    return capstones or [NULL]\n",
        "\n",
        "\n",
        "def predict_probs(context_tokens: list[str]) -> torch.Tensor:\n",
        "    x = torch.tensor([[vocab[t] for t in context_tokens]], device=device)\n",
        "    w = torch.tensor([[weapon_vocab[weapon_id]]], device=device)\n",
        "    mask = x == pad_token_id\n",
        "    with torch.inference_mode():\n",
        "        probs = torch.sigmoid(model(x, w, key_padding_mask=mask)).squeeze(0)\n",
        "    return probs.detach().cpu()\n",
        "\n",
        "\n",
        "def predict_top_tokens(context_tokens: list[str], k: int = 15):\n",
        "    probs_cpu = predict_probs(context_tokens)\n",
        "    skip = {vocab.get(PAD), vocab.get(NULL)}\n",
        "    top = torch.topk(probs_cpu, k=min(k, probs_cpu.numel()))\n",
        "    out = []\n",
        "    for idx, p in zip(top.indices.tolist(), top.values.tolist()):\n",
        "        tok = inv_vocab[idx]\n",
        "        if tok.startswith(\"<\") or idx in skip:\n",
        "            continue\n",
        "        out.append((tok, float(p)))\n",
        "    return out\n",
        "\n",
        "\n",
        "def top_delta_tokens(delta_vec: torch.Tensor, k: int, *, largest: bool):\n",
        "    skip = {vocab.get(PAD), vocab.get(NULL)}\n",
        "    values, indices = torch.topk(\n",
        "        delta_vec if largest else -delta_vec,\n",
        "        k=min(k, delta_vec.numel()),\n",
        "    )\n",
        "    out = []\n",
        "    for idx, v in zip(indices.tolist(), values.tolist()):\n",
        "        tok = inv_vocab[idx]\n",
        "        if tok.startswith(\"<\") or idx in skip:\n",
        "            continue\n",
        "        dv = float(v) if largest else -float(v)\n",
        "        out.append((tok, dv))\n",
        "    return out\n",
        "\n",
        "\n",
        "baseline_tokens = [NULL]\n",
        "partial_tokens = tokenize_build(partial_build)\n",
        "capstone_tokens = build_to_capstones(partial_build)\n",
        "\n",
        "print(\"weapon_id:\", weapon_id)\n",
        "print(\"partial_build:\", partial_build)\n",
        "print(\"partial_tokens (cumulative):\", partial_tokens)\n",
        "print(\"capstone_tokens (for beam search):\", capstone_tokens)\n",
        "\n",
        "if SHOW_TOKEN_TABLES:\n",
        "    print(\"\\nTop predictions from <NULL> (baseline):\")\n",
        "    for tok, p in predict_top_tokens(baseline_tokens, k=12):\n",
        "        print(f\"{tok:<32} {p:.4f}\")\n",
        "\n",
        "    print(\"\\nTop predictions from partial build tokens:\")\n",
        "    for tok, p in predict_top_tokens(partial_tokens, k=12):\n",
        "        print(f\"{tok:<32} {p:.4f}\")\n",
        "\n",
        "    baseline_probs = predict_probs(baseline_tokens)\n",
        "    partial_probs = predict_probs(partial_tokens)\n",
        "    delta = partial_probs - baseline_probs\n",
        "\n",
        "    print(\"\\nTop +\u0394 tokens (partial - baseline):\")\n",
        "    for tok, dv in top_delta_tokens(delta, k=12, largest=True):\n",
        "        print(f\"{tok:<32} {dv:+.4f}\")\n",
        "\n",
        "    print(\"\\nTop -\u0394 tokens (partial - baseline):\")\n",
        "    for tok, dv in top_delta_tokens(delta, k=12, largest=False):\n",
        "        print(f\"{tok:<32} {dv:+.4f}\")\n",
        "else:\n",
        "    print(\"\\n(Set SHOW_TOKEN_TABLES=True to print token-space tables.)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Beam search: token-space \u2192 legal build-space\n",
        "\n",
        "Interpretation note: this notebook shows **recommendation-style** decoding.\n",
        "The decoder is free to choose any slot assignment that yields a legal build\n",
        "consistent with the requested ability-point hints (it does *not* freeze specific\n",
        "gear pieces).\n",
        "\n",
        "We run two demos:\n",
        "\n",
        "- Baseline build from `<NULL>` only\n",
        "- Completion given a partial build (capstone tokens)\n",
        "\n",
        "This uses the repo\u2019s constraint-aware reconstruction (allocator + beam search).\n",
        "\n",
        "Tip: set `SHOW_TRACE = True` to print a compact step-by-step trace.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from splatnlp.utils.reconstruct.allocator import Allocator\n",
        "from splatnlp.utils.reconstruct.beam_search import reconstruct_build\n",
        "\n",
        "import time\n",
        "\n",
        "allocator = Allocator()\n",
        "vocab_size = len(vocab)\n",
        "inv_vocab_list = [inv_vocab[i] for i in range(vocab_size)]\n",
        "\n",
        "def predict_fn(current_tokens: list[str], weapon_id: str):\n",
        "    ids = [vocab[t] for t in current_tokens]\n",
        "    x = torch.tensor([ids], device=device)\n",
        "    w = torch.tensor([[weapon_vocab[weapon_id]]], device=device)\n",
        "    mask = x == pad_token_id\n",
        "    with torch.inference_mode():\n",
        "        probs = torch.sigmoid(model(x, w, key_padding_mask=mask)).squeeze(0)\n",
        "    probs = probs.detach().cpu().tolist()\n",
        "    return {inv_vocab_list[i]: float(probs[i]) for i in range(vocab_size)}\n",
        "\n",
        "def summarize_trace(trace, top_preds: int = 8) -> None:\n",
        "    seen = set()\n",
        "    for fr in trace:\n",
        "        caps = sorted(fr.partial_caps.keys())\n",
        "        added = sorted(set(caps) - seen)\n",
        "        seen.update(caps)\n",
        "        top = sorted(fr.logits.items(), key=lambda kv: kv[1], reverse=True)\n",
        "        top = [(t, round(p, 4)) for t, p in top if not t.startswith(\"<\")][:top_preds]\n",
        "        print({\"step\": fr.step, \"added\": added, \"top_preds\": top})\n",
        "\n",
        "def print_build(label: str, build) -> None:\n",
        "    d = build.to_dict()\n",
        "    achieved = dict(\n",
        "        sorted(d[\"achieved_ap\"].items(), key=lambda kv: kv[1], reverse=True)\n",
        "    )\n",
        "    print(f\"\\n{label}\")\n",
        "    print(\"total_ap:\", d[\"total_ap\"])\n",
        "    print(\"mains:\", d[\"mains\"])\n",
        "    print(\"subs:\", d[\"subs\"])\n",
        "    print(\"achieved_ap:\", achieved)\n",
        "\n",
        "SHOW_TRACE = False\n",
        "\n",
        "def run_beam(label: str, initial_context: list[str]):\n",
        "    start = time.perf_counter()\n",
        "    if SHOW_TRACE:\n",
        "        builds, traces = reconstruct_build(\n",
        "            predict_fn=predict_fn,\n",
        "            weapon_id=weapon_id,\n",
        "            initial_context=initial_context,\n",
        "            allocator=allocator,\n",
        "            beam_size=5,\n",
        "            max_steps=6,\n",
        "            top_k=1,\n",
        "            record_traces=True,\n",
        "        )\n",
        "    else:\n",
        "        builds = reconstruct_build(\n",
        "            predict_fn=predict_fn,\n",
        "            weapon_id=weapon_id,\n",
        "            initial_context=initial_context,\n",
        "            allocator=allocator,\n",
        "            beam_size=5,\n",
        "            max_steps=6,\n",
        "            top_k=1,\n",
        "            record_traces=False,\n",
        "        )\n",
        "        traces = None\n",
        "\n",
        "    elapsed = time.perf_counter() - start\n",
        "    print(f\"{label} runtime: {elapsed:.3f}s\")\n",
        "\n",
        "    if not builds:\n",
        "        raise RuntimeError(f\"No valid build produced for: {label}\")\n",
        "\n",
        "    build = builds[0]\n",
        "    print_build(label, build)\n",
        "\n",
        "    if SHOW_TRACE:\n",
        "        summarize_trace(traces[0])\n",
        "\n",
        "    return build\n",
        "\n",
        "baseline_build = run_beam(\"Baseline build (<NULL>)\", [NULL])\n",
        "completion_build = run_beam(\"Completion from partial capstones\", capstone_tokens)\n",
        "\n",
        "def print_ap_delta(a, b, *, max_rows: int = 15) -> None:\n",
        "    a_ap = a.to_dict()[\"achieved_ap\"]\n",
        "    b_ap = b.to_dict()[\"achieved_ap\"]\n",
        "    rows = []\n",
        "    for ability in sorted(set(a_ap) | set(b_ap)):\n",
        "        av = int(a_ap.get(ability, 0))\n",
        "        bv = int(b_ap.get(ability, 0))\n",
        "        if av == bv:\n",
        "            continue\n",
        "        rows.append((ability, av, bv, bv - av))\n",
        "\n",
        "    rows.sort(key=lambda r: abs(r[3]), reverse=True)\n",
        "    print(\"\\nAP changes (completion - baseline):\")\n",
        "    for ability, av, bv, dv in rows[:max_rows]:\n",
        "        print(f\"{ability:<24} {av:>2} -> {bv:>2} ({dv:+d})\")\n",
        "\n",
        "def check_constraints(build, requested: dict[str, int]) -> None:\n",
        "    achieved = build.to_dict()[\"achieved_ap\"]\n",
        "    ok = True\n",
        "    print(\"\\nConstraint check (completion build):\")\n",
        "    for ability, req in requested.items():\n",
        "        got = int(achieved.get(ability, 0))\n",
        "        met = got >= int(req)\n",
        "        ok = ok and met\n",
        "        status = \"OK\" if met else \"FAIL\"\n",
        "        print(f\"{ability:<24} req={req:>2} got={got:>2} {status}\")\n",
        "    print(\"all_met:\", ok)\n",
        "\n",
        "print_ap_delta(baseline_build, completion_build)\n",
        "check_constraints(completion_build, partial_build)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build \u2194 token visualization\n",
        "\n",
        "This renders a 57-AP build as **3 mains + 9 subs**, then shows how it collapses to token-space:\n",
        "\n",
        "- **Beam-search capstones** (highest tier per ability family)\n",
        "- **Cumulative threshold tokens** (what the model actually sees)\n",
        "\n",
        "Sub-slot ordering is not unique; this is just a display assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Pretty build visualization (run to render)\n",
        "from collections import Counter\n",
        "import html as _html\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "from splatnlp.serve.tokenize import tokenize_build\n",
        "from splatnlp.utils.constants import (\n",
        "    BUCKET_THRESHOLDS,\n",
        "    MAIN_ONLY_ABILITIES,\n",
        "    NULL,\n",
        "    STANDARD_ABILITIES,\n",
        ")\n",
        "\n",
        "def _abbrev(token: str) -> str:\n",
        "    special = {\n",
        "        \"ink_recovery_up\": \"iru\",\n",
        "        \"ink_resistance_up\": \"res\",\n",
        "        \"ink_saver_main\": \"ism\",\n",
        "        \"ink_saver_sub\": \"iss\",\n",
        "        \"quick_respawn\": \"qr\",\n",
        "        \"quick_super_jump\": \"qsj\",\n",
        "        \"run_speed_up\": \"rsu\",\n",
        "        \"sub_resistance_up\": \"sru\",\n",
        "        \"special_charge_up\": \"scu\",\n",
        "        \"special_power_up\": \"spu\",\n",
        "        \"special_saver\": \"ss\",\n",
        "        \"sub_power_up\": \"bpu\",\n",
        "        \"swim_speed_up\": \"ssu\",\n",
        "        \"intensify_action\": \"ia\",\n",
        "    }\n",
        "    base = token\n",
        "    suffix = \"\"\n",
        "    if \"_\" in token and token.rsplit(\"_\", 1)[-1].isdigit():\n",
        "        base, suffix = token.rsplit(\"_\", 1)\n",
        "    short = special.get(base) or \"\".join(w[0] for w in base.split(\"_\"))\n",
        "    return f\"{short}_{suffix}\" if suffix else short\n",
        "\n",
        "def _pretty(token: str) -> str:\n",
        "    return token.replace(\"_\", \" \").title()\n",
        "\n",
        "def _ap_to_capstones(ap: dict[str, int]) -> list[str]:\n",
        "    caps: list[str] = []\n",
        "    for ability in MAIN_ONLY_ABILITIES:\n",
        "        if int(ap.get(ability, 0) or 0) > 0:\n",
        "            caps.append(ability)\n",
        "    for ability in STANDARD_ABILITIES:\n",
        "        val = int(ap.get(ability, 0) or 0)\n",
        "        thresh = max(\n",
        "            (t for t in BUCKET_THRESHOLDS if t <= val),\n",
        "            default=None,\n",
        "        )\n",
        "        if thresh is not None:\n",
        "            caps.append(f\"{ability}_{thresh}\")\n",
        "    return caps or [NULL]\n",
        "\n",
        "def _palette(n: int) -> list[str]:\n",
        "    base = [\n",
        "        \"#4c78a8\",\n",
        "        \"#f58518\",\n",
        "        \"#54a24b\",\n",
        "        \"#e45756\",\n",
        "        \"#72b7b2\",\n",
        "        \"#b279a2\",\n",
        "        \"#ff9da6\",\n",
        "        \"#9d755d\",\n",
        "        \"#bab0ac\",\n",
        "        \"#1f77b4\",\n",
        "        \"#ff7f0e\",\n",
        "        \"#2ca02c\",\n",
        "        \"#d62728\",\n",
        "        \"#9467bd\",\n",
        "    ]\n",
        "    if n <= len(base):\n",
        "        return base[:n]\n",
        "    return [base[i % len(base)] for i in range(n)]\n",
        "\n",
        "def _build_color_map(*build_dicts: dict) -> dict[str, str]:\n",
        "    fams: set[str] = set()\n",
        "    for d in build_dicts:\n",
        "        fams.update(d.get(\"achieved_ap\", {}).keys())\n",
        "        for v in d.get(\"mains\", {}).values():\n",
        "            if v is not None:\n",
        "                fams.add(v)\n",
        "        fams.update(d.get(\"subs\", {}).keys())\n",
        "    fam_list = sorted(fams)\n",
        "    cols = _palette(len(fam_list))\n",
        "    return {fam: col for fam, col in zip(fam_list, cols)}\n",
        "\n",
        "def _expand_subs(subs: dict[str, int]) -> list[str | None]:\n",
        "    items = []\n",
        "    for fam, c in sorted(\n",
        "        subs.items(),\n",
        "        key=lambda kv: (-int(kv[1]), kv[0]),\n",
        "    ):\n",
        "        items.extend([fam] * int(c))\n",
        "    if len(items) < 9:\n",
        "        items.extend([None] * (9 - len(items)))\n",
        "    return items[:9]\n",
        "\n",
        "def _render_build_card(title: str, build, *, colors: dict[str, str]) -> str:\n",
        "    d = build.to_dict()\n",
        "    mains = d[\"mains\"]\n",
        "    subs = _expand_subs(d[\"subs\"])\n",
        "    subs_by_slot = {\n",
        "        \"head\": subs[0:3],\n",
        "        \"clothes\": subs[3:6],\n",
        "        \"shoes\": subs[6:9],\n",
        "    }\n",
        "\n",
        "    ap = {k: int(v) for k, v in d[\"achieved_ap\"].items()}\n",
        "    cumulative = tokenize_build(ap)\n",
        "    capstones = _ap_to_capstones(ap)\n",
        "\n",
        "    def slot_box(label: str | None, kind: str) -> str:\n",
        "        if label is None:\n",
        "            return f\"<div class='slot {kind} empty'>empty</div>\"\n",
        "        col = colors.get(label, \"#eee\")\n",
        "        display_name = _html.escape(_pretty(label))\n",
        "        short = _html.escape(_abbrev(label))\n",
        "        full = _html.escape(label)\n",
        "        return (\n",
        "            f\"<div class='slot {kind}' title='{full}' \"\n",
        "            f\"style='background:{col}30;border-color:{col}88'>\"\n",
        "            f\"<div class='slot-name'>{display_name}</div>\"\n",
        "            f\"<div class='slot-abbrev'>{short}</div>\"\n",
        "            \"</div>\"\n",
        "        )\n",
        "\n",
        "    gear_cols = []\n",
        "    for slot_name in [\"head\", \"clothes\", \"shoes\"]:\n",
        "        main = mains.get(slot_name)\n",
        "        col_html = [\n",
        "            \"<div class='gear-col'>\",\n",
        "            f\"<div class='gear-title'>{slot_name.title()}</div>\",\n",
        "            slot_box(main, \"main\"),\n",
        "        ]\n",
        "        for sub in subs_by_slot[slot_name]:\n",
        "            col_html.append(slot_box(sub, \"sub\"))\n",
        "        col_html.append(\"</div>\")\n",
        "        gear_cols.append(\"\\n\".join(col_html))\n",
        "\n",
        "    main_counts = Counter([m for m in mains.values() if m is not None])\n",
        "    rows = []\n",
        "    for fam in sorted(ap.keys()):\n",
        "        ap_val = int(ap[fam])\n",
        "        m = int(main_counts.get(fam, 0))\n",
        "        s = int(d[\"subs\"].get(fam, 0))\n",
        "        col = colors.get(fam, \"#eee\")\n",
        "\n",
        "        toks = []\n",
        "        cap = None\n",
        "        if fam in MAIN_ONLY_ABILITIES:\n",
        "            toks = [fam]\n",
        "            cap = fam\n",
        "        elif fam in STANDARD_ABILITIES:\n",
        "            toks = [f\"{fam}_{t}\" for t in BUCKET_THRESHOLDS if t <= ap_val]\n",
        "            cap = toks[-1] if toks else None\n",
        "\n",
        "        chip_html = []\n",
        "        for t in toks:\n",
        "            short = _html.escape(_abbrev(t))\n",
        "            full = _html.escape(t)\n",
        "            cls = \"chip cap\" if t == cap else \"chip\"\n",
        "            bg = f\"{col}26\" if t == cap else f\"{col}14\"\n",
        "            chip_html.append(\n",
        "                f\"<span class='{cls}' title='{full}' \"\n",
        "                f\"style='border-color:{col}99;background:{bg}'>{short}</span>\"\n",
        "            )\n",
        "\n",
        "        rows.append(\n",
        "            \"\\n\".join(\n",
        "                [\n",
        "                    \"<tr>\",\n",
        "                    (\n",
        "                        \"<td>\"\n",
        "                        f\"<span class='fam' title='{_html.escape(fam)}' \"\n",
        "                        f\"style='background:{col}22;border-color:{col}88'>\"\n",
        "                        f\"{_html.escape(_abbrev(fam).upper())}</span>\"\n",
        "                        f\"<span class='fam-full' title='{_html.escape(fam)}'>\"\n",
        "                        f\"{_html.escape(_pretty(fam))}</span>\"\n",
        "                        \"</td>\"\n",
        "                    ),\n",
        "                    f\"<td>{m}</td>\",\n",
        "                    f\"<td>{s}</td>\",\n",
        "                    f\"<td>{ap_val}</td>\",\n",
        "                    f\"<td>{''.join(chip_html) if chip_html else ''}</td>\",\n",
        "                    \"</tr>\",\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def list_as_code(items: list[str]) -> str:\n",
        "        safe = \", \".join(_html.escape(x) for x in items)\n",
        "        return f\"<code>{safe}</code>\"\n",
        "\n",
        "    cap_n = len([t for t in capstones if t != NULL])\n",
        "    cum_n = len([t for t in cumulative if t != NULL])\n",
        "    return \"\\n\".join(\n",
        "        [\n",
        "            \"<div class='flow-card'>\",\n",
        "            f\"<div class='flow-title'>{_html.escape(title)}</div>\",\n",
        "            (\n",
        "                f\"<div class='small'>total_ap={int(d['total_ap'])} | \"\n",
        "                f\"capstones={cap_n} | cumulative_tokens={cum_n}</div>\"\n",
        "            ),\n",
        "            \"<div class='gear-grid'>\",\n",
        "            *gear_cols,\n",
        "            \"</div>\",\n",
        "            \"<details class='details'><summary>Beam-search capstones</summary>\",\n",
        "            list_as_code(capstones),\n",
        "            \"</details>\",\n",
        "            \"<details class='details'><summary>Tokens fed to model (cumulative)</summary>\",\n",
        "            list_as_code(cumulative),\n",
        "            \"</details>\",\n",
        "            \"<div class='small' style='margin-top:8px'>\",\n",
        "            \"Per-family breakdown (capstone token is bold):\",\n",
        "            \"</div>\",\n",
        "            \"<table class='tbl'>\",\n",
        "            \"<thead><tr><th>ability</th><th>m</th><th>s</th><th>AP</th><th>tokens</th></tr></thead>\",\n",
        "            \"<tbody>\",\n",
        "            *rows,\n",
        "            \"</tbody></table>\",\n",
        "            \"<div class='small note'>\",\n",
        "            \"Note: sub-slot order is not unique; this is a display assignment.\",\n",
        "            \"</div>\",\n",
        "            \"</div>\",\n",
        "        ]\n",
        "    )\n",
        "\n",
        "STYLE = \"\"\"\n",
        "<style>\n",
        ".flow-wrap{font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Arial;}\n",
        ".flow-row{display:flex;gap:14px;flex-wrap:wrap;align-items:flex-start;}\n",
        ".flow-card{border:1px solid #e6e6e6;border-radius:14px;padding:12px;width:520px;box-shadow:0 1px 0 rgba(0,0,0,0.02);}\n",
        ".flow-title{font-weight:700;font-size:14px;margin-bottom:2px;}\n",
        ".small{color:#555;font-size:12px;}\n",
        ".gear-grid{display:grid;grid-template-columns:repeat(3,1fr);gap:10px;margin-top:10px;}\n",
        ".gear-col{background:#fafafa;border:1px solid #eee;border-radius:12px;padding:8px;}\n",
        ".gear-title{font-weight:600;font-size:12px;color:#333;margin-bottom:6px;}\n",
        ".slot{border-radius:10px;padding:7px 8px;margin-top:6px;border:1px solid rgba(0,0,0,0.10);}\n",
        ".slot-name{font-size:12px;font-weight:800;line-height:1.15;}\n",
        ".slot.main .slot-name{font-size:13px;}\n",
        ".slot.sub .slot-name{font-size:12px;font-weight:700;}\n",
        ".slot-abbrev{margin-top:2px;font-size:11px;color:#222;opacity:0.85;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,\"Liberation Mono\",\"Courier New\",monospace;}\n",
        ".slot.empty{background:#f0f0f0;color:#888;border-color:#e0e0e0;}\n",
        ".details{margin-top:10px;}\n",
        ".details summary{cursor:pointer;color:#333;font-size:12px;font-weight:600;}\n",
        "code{background:#f6f6f6;border:1px solid #eee;padding:2px 6px;border-radius:8px;font-size:12px;}\n",
        ".tbl{width:100%;border-collapse:collapse;margin-top:6px;}\n",
        ".tbl th,.tbl td{border-bottom:1px solid #eee;padding:4px 6px;font-size:12px;vertical-align:top;}\n",
        ".tbl th{color:#333;text-align:left;font-weight:700;}\n",
        ".fam{display:inline-block;padding:2px 8px;border-radius:999px;border:1px solid rgba(0,0,0,0.12);font-weight:800;font-size:11px;}\n",
        ".fam-full{margin-left:6px;font-size:12px;color:#222;font-weight:600;}\n",
        ".chip{display:inline-block;margin:2px 4px 2px 0;padding:2px 8px;border-radius:999px;border:1px solid rgba(0,0,0,0.12);font-size:12px;}\n",
        ".chip.cap{border-width:2px;font-weight:800;}\n",
        ".note{margin-top:8px;}\n",
        "</style>\n",
        "\"\"\"\n",
        "\n",
        "baseline_d = baseline_build.to_dict()\n",
        "completion_d = completion_build.to_dict()\n",
        "color_map = _build_color_map(baseline_d, completion_d)\n",
        "\n",
        "html_out = \"\\n\".join(\n",
        "    [\n",
        "        \"<div class='flow-wrap'>\",\n",
        "        STYLE,\n",
        "        \"<div class='flow-row'>\",\n",
        "        _render_build_card(\"Baseline build\", baseline_build, colors=color_map),\n",
        "        _render_build_card(\"Completion build\", completion_build, colors=color_map),\n",
        "        \"</div>\",\n",
        "        \"</div>\",\n",
        "    ]\n",
        ")\n",
        "display(HTML(html_out))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ultra + SAE hook introspection (probe)\n",
        "\n",
        "This section (optional) loads `model_ultra.pth` + `sae_model_ultra.pth`, registers\n",
        "a hook on the pooled 512-D vector, then:\n",
        "\n",
        "- checks SAE reconstruction quality (offline; model outputs unchanged)\n",
        "- prints top active SAE features (optionally labeled)\n",
        "- shows which tokens a feature most influences\n",
        "\n",
        "Requires `ENABLE_ULTRA_SAE=True` in the download section above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Toggle: set False for a faster notebook (skips SAE section).\n",
        "RUN_ULTRA_SAE = bool(globals().get(\"ENABLE_ULTRA_SAE\", True))\n",
        "\n",
        "if not RUN_ULTRA_SAE:\n",
        "    print(\"Skipping Ultra SAE section (RUN_ULTRA_SAE=False).\")\n",
        "else:\n",
        "    required = [\n",
        "        \"model_ultra.pth\",\n",
        "        \"sae_config_ultra.json\",\n",
        "        \"sae_model_ultra.pth\",\n",
        "    ]\n",
        "    missing = [name for name in required if not (OUT_DIR / name).exists()]\n",
        "    if missing:\n",
        "        raise FileNotFoundError(\n",
        "            \"Missing Ultra artifacts: \"\n",
        "            + \", \".join(missing)\n",
        "            + \". Re-run the download cell (ENABLE_ULTRA_SAE=True).\"\n",
        "        )\n",
        "\n",
        "    import json\n",
        "    import textwrap\n",
        "\n",
        "    import torch\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "    from splatnlp.model.models import SetCompletionModel\n",
        "\n",
        "    from splatnlp.monosemantic_sae.hooks import register_hooks\n",
        "    from splatnlp.monosemantic_sae.models import SparseAutoencoder\n",
        "    from splatnlp.utils.constants import NULL, PAD\n",
        "    from splatnlp.utils.reconstruct.allocator import Allocator\n",
        "    from splatnlp.utils.reconstruct.beam_search import reconstruct_build\n",
        "\n",
        "    # Reuse the already-loaded model if it's Ultra.\n",
        "    use_ultra_loaded = (\n",
        "        bool(globals().get(\"USE_ULTRA_MODEL\", False))\n",
        "        and globals().get(\"MODEL_FILENAME\") == \"model_ultra.pth\"\n",
        "        and \"model\" in globals()\n",
        "    )\n",
        "\n",
        "    if use_ultra_loaded:\n",
        "        ultra = model\n",
        "    else:\n",
        "        ultra = SetCompletionModel(**params)\n",
        "        ultra.load_state_dict(\n",
        "            torch.load(OUT_DIR / \"model_ultra.pth\", map_location=\"cpu\")\n",
        "        )\n",
        "        ultra.to(device)\n",
        "        ultra.eval()\n",
        "\n",
        "    sae_cfg = json.loads((OUT_DIR / \"sae_config_ultra.json\").read_text())\n",
        "    sae = SparseAutoencoder(\n",
        "        input_dim=int(sae_cfg.get(\"input_dim\", 512)),\n",
        "        expansion_factor=float(sae_cfg.get(\"expansion_factor\", 48.0)),\n",
        "        l1_coefficient=float(sae_cfg.get(\"l1_coefficient\", 0.0)),\n",
        "        target_usage=float(sae_cfg.get(\"target_usage\", 0.0)),\n",
        "        usage_coeff=float(sae_cfg.get(\"usage_coeff\", 0.0)),\n",
        "        dead_neuron_threshold=float(sae_cfg.get(\"dead_neuron_threshold\", 1e-6)),\n",
        "        dead_neuron_steps=int(sae_cfg.get(\"dead_neuron_steps\", 12500)),\n",
        "    )\n",
        "    sae.load_state_dict(\n",
        "        torch.load(OUT_DIR / \"sae_model_ultra.pth\", map_location=\"cpu\")\n",
        "    )\n",
        "    sae.to(device)\n",
        "    sae.eval()\n",
        "\n",
        "    dec_w = sae.decoder.weight\n",
        "    model_dim = ultra.output_layer.weight.shape[1]\n",
        "    n_feats = (\n",
        "        int(dec_w.shape[1])\n",
        "        if int(dec_w.shape[0]) == int(model_dim)\n",
        "        else int(dec_w.shape[0])\n",
        "    )\n",
        "    print(\n",
        "        f\"SAE: input_dim={sae_cfg.get('input_dim')} | \"\n",
        "        f\"expansion={sae_cfg.get('expansion_factor')} | \"\n",
        "        f\"feats\u2248{n_feats}\"\n",
        "    )\n",
        "\n",
        "    # Probe mode: capture SAE activations without changing model outputs.\n",
        "    hook, handle = register_hooks(\n",
        "        ultra,\n",
        "        sae_model=sae,\n",
        "        bypass=False,\n",
        "        no_change=True,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        vocab_size = len(vocab)\n",
        "        inv_vocab_list = [inv_vocab[i] for i in range(vocab_size)]\n",
        "        skip_ids = {vocab.get(PAD), vocab.get(NULL)}\n",
        "\n",
        "        def _compress_ws(text: str) -> str:\n",
        "            return \" \".join((text or \"\").strip().split())\n",
        "\n",
        "        def _wrap_notes(text: str, *, width: int = 80, indent: int = 6) -> str:\n",
        "            compressed = _compress_ws(text)\n",
        "            if not compressed:\n",
        "                return \"\"\n",
        "            wrapped = textwrap.fill(compressed, width=width)\n",
        "            return textwrap.indent(wrapped, \" \" * indent)\n",
        "\n",
        "        def load_ultra_feature_labels() -> dict[int, dict[str, str]]:\n",
        "            candidates = [\n",
        "                OUT_DIR / \"consolidated_ultra.json\",\n",
        "                OUT_DIR / \"feature_labels_ultra.json\",\n",
        "            ]\n",
        "            for path in candidates:\n",
        "                if not path.exists():\n",
        "                    continue\n",
        "                raw = json.loads(path.read_text())\n",
        "                if not isinstance(raw, dict):\n",
        "                    continue\n",
        "\n",
        "                consolidated = any(\n",
        "                    isinstance(v, dict)\n",
        "                    and (\n",
        "                        \"feature_id\" in v\n",
        "                        or \"display_name\" in v\n",
        "                        or \"dashboard_name\" in v\n",
        "                        or \"dashboard_notes\" in v\n",
        "                    )\n",
        "                    for v in raw.values()\n",
        "                )\n",
        "\n",
        "                labels: dict[int, dict[str, str]] = {}\n",
        "                if consolidated:\n",
        "                    for k, v in raw.items():\n",
        "                        if not isinstance(v, dict):\n",
        "                            continue\n",
        "                        fid = v.get(\"feature_id\")\n",
        "                        if fid is None:\n",
        "                            try:\n",
        "                                fid = int(k)\n",
        "                            except (TypeError, ValueError):\n",
        "                                continue\n",
        "                        labels[int(fid)] = {\n",
        "                            \"name\": v.get(\"display_name\")\n",
        "                            or v.get(\"dashboard_name\")\n",
        "                            or \"\",\n",
        "                            \"category\": v.get(\"dashboard_category\") or \"none\",\n",
        "                            \"notes\": v.get(\"dashboard_notes\") or \"\",\n",
        "                        }\n",
        "                    return labels\n",
        "\n",
        "                for k, v in raw.items():\n",
        "                    if not isinstance(v, dict):\n",
        "                        continue\n",
        "                    try:\n",
        "                        fid = int(k)\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "                    labels[fid] = {\n",
        "                        \"name\": v.get(\"name\") or \"\",\n",
        "                        \"category\": v.get(\"category\") or \"none\",\n",
        "                        \"notes\": v.get(\"notes\") or \"\",\n",
        "                    }\n",
        "                return labels\n",
        "\n",
        "            return {}\n",
        "\n",
        "        feature_labels = load_ultra_feature_labels()\n",
        "        if feature_labels:\n",
        "            print(\"Loaded feature labels:\", len(feature_labels))\n",
        "\n",
        "        def forward_probs(tokens: list[str], weapon_id: str) -> torch.Tensor:\n",
        "            ids = [vocab[t] for t in tokens]\n",
        "            x = torch.tensor([ids], device=device)\n",
        "            w = torch.tensor([[weapon_vocab[weapon_id]]], device=device)\n",
        "            mask = x == pad_token_id\n",
        "            with torch.inference_mode():\n",
        "                probs = torch.sigmoid(\n",
        "                    ultra(x, w, key_padding_mask=mask)\n",
        "                ).squeeze(0)\n",
        "            return probs.detach().cpu()\n",
        "\n",
        "        def predict_probs_dict(current_tokens: list[str], weapon_id: str):\n",
        "            probs = forward_probs(current_tokens, weapon_id).tolist()\n",
        "            return {inv_vocab_list[i]: float(probs[i]) for i in range(vocab_size)}\n",
        "\n",
        "        def predict_probs_and_acts(current_tokens: list[str], weapon_id: str):\n",
        "            probs_dict = predict_probs_dict(current_tokens, weapon_id)\n",
        "            acts = (\n",
        "                hook.last_h_post.detach().cpu().flatten()\n",
        "                if hook.last_h_post is not None\n",
        "                else None\n",
        "            )\n",
        "            return probs_dict, acts\n",
        "\n",
        "        # Offline reconstruction quality check (does not change model outputs).\n",
        "        _ = forward_probs([NULL], weapon_id)\n",
        "        if hook.last_in is not None and hook.last_h_post is not None:\n",
        "            with torch.inference_mode():\n",
        "                x_in = hook.last_in.detach()\n",
        "                h_post = hook.last_h_post.detach()\n",
        "                x_recon = sae.decode(h_post)\n",
        "                mse = F.mse_loss(x_recon, x_in).item()\n",
        "                cos = F.cosine_similarity(\n",
        "                    x_recon.flatten(),\n",
        "                    x_in.flatten(),\n",
        "                    dim=0,\n",
        "                ).item()\n",
        "            print(\"SAE recon (offline) MSE:\", float(mse))\n",
        "            print(\"SAE recon (offline) cosine:\", float(cos))\n",
        "\n",
        "        # Run a traced beam search in probe mode so we can see activations.\n",
        "        builds, traces = reconstruct_build(\n",
        "            predict_fn=predict_probs_and_acts,\n",
        "            weapon_id=weapon_id,\n",
        "            initial_context=[NULL],\n",
        "            allocator=Allocator(),\n",
        "            beam_size=5,\n",
        "            max_steps=6,\n",
        "            top_k=1,\n",
        "            record_traces=True,\n",
        "        )\n",
        "\n",
        "        if not builds or not traces:\n",
        "            raise RuntimeError(\"No build/trace produced\")\n",
        "\n",
        "        print(\"\\nUltra build (probe mode):\")\n",
        "        print(builds[0].to_dict())\n",
        "\n",
        "        acts = traces[0][-1].activations\n",
        "        if acts is None:\n",
        "            raise RuntimeError(\"No SAE activations captured\")\n",
        "\n",
        "        SHOW_LABEL_NOTES = False\n",
        "        MAX_NOTE_CHARS = 220\n",
        "\n",
        "        topk = torch.topk(acts, k=10)\n",
        "        print(\"\\nTop active SAE features (probe mode):\")\n",
        "        for i, v in zip(topk.indices.tolist(), topk.values.tolist()):\n",
        "            fid = int(i)\n",
        "            val = float(v)\n",
        "            meta = feature_labels.get(fid)\n",
        "            if not meta:\n",
        "                print(fid, val)\n",
        "                continue\n",
        "            cat = (meta.get(\"category\") or \"none\").strip()\n",
        "            name = (meta.get(\"name\") or \"\").strip()\n",
        "            label = f\"[{cat}] {name}\".strip() if name else f\"[{cat}]\"\n",
        "            print(f\"{fid:5d} {val:.4f} {label}\")\n",
        "            if SHOW_LABEL_NOTES:\n",
        "                notes = _wrap_notes(meta.get(\"notes\", \"\"))\n",
        "                if notes:\n",
        "                    if len(notes) > MAX_NOTE_CHARS:\n",
        "                        notes = notes[: MAX_NOTE_CHARS - 1] + \"\u2026\"\n",
        "                    print(notes)\n",
        "\n",
        "        def pick_feature(feature_ids: list[int]) -> int:\n",
        "            preferred: list[int] = []\n",
        "            fallback: list[int] = []\n",
        "            for fid in feature_ids:\n",
        "                meta = feature_labels.get(int(fid))\n",
        "                if not meta:\n",
        "                    continue\n",
        "                name = (meta.get(\"name\") or \"\").strip()\n",
        "                cat = (meta.get(\"category\") or \"\").strip().lower()\n",
        "                if not name:\n",
        "                    continue\n",
        "                if cat in {\"strategic\", \"tactical\"}:\n",
        "                    preferred.append(int(fid))\n",
        "                else:\n",
        "                    fallback.append(int(fid))\n",
        "            if preferred:\n",
        "                return preferred[0]\n",
        "            if fallback:\n",
        "                return fallback[0]\n",
        "            return int(feature_ids[0])\n",
        "\n",
        "        feature_id = pick_feature([int(i) for i in topk.indices.tolist()])\n",
        "        feature_value = float(acts[feature_id])\n",
        "        print(f\"\\nExample feature: {feature_id} (value={feature_value:.4f})\")\n",
        "\n",
        "        dec_w = sae.decoder.weight\n",
        "        model_dim = ultra.output_layer.weight.shape[1]\n",
        "        if int(dec_w.shape[0]) != int(model_dim):\n",
        "            dec_w = dec_w.T\n",
        "\n",
        "        decoder_norm = F.normalize(dec_w, dim=0)\n",
        "        influence = ultra.output_layer.weight @ decoder_norm[:, feature_id]\n",
        "        influence = influence.detach().cpu().tolist()\n",
        "\n",
        "        infl_pairs = []\n",
        "        for i, val in enumerate(influence):\n",
        "            tok = inv_vocab_list[i]\n",
        "            if tok.startswith(\"<\") or i in skip_ids:\n",
        "                continue\n",
        "            infl_pairs.append((tok, float(val)))\n",
        "        infl_pairs.sort(key=lambda kv: kv[1], reverse=True)\n",
        "\n",
        "        print(\"Top + tokens by logit influence:\")\n",
        "        for tok, val in infl_pairs[:10]:\n",
        "            print(f\"{tok:<32} {val:+.4f}\")\n",
        "\n",
        "        print(\"\\nTop - tokens by logit influence:\")\n",
        "        for tok, val in sorted(infl_pairs, key=lambda kv: kv[1])[:10]:\n",
        "            print(f\"{tok:<32} {val:+.4f}\")\n",
        "\n",
        "    finally:\n",
        "        handle.remove()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "SplatNLP_colab_demo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "splatnlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
